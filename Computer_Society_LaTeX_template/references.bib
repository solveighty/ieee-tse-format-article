% This file was created with Mendeley

@article{Trudova2020,
   abstract = {Artificial intelligence (AI) has made a considerable impact on the software engineering field, and the area of software testing is not an exception. In theory, AI techniques could help to achieve the highest possible level of software test automation. The goal of this Systematic Literature Review (SLR) paper is to highlight the role of artificial intelligence in the software test automation area through cataloguing AI techniques and related software testing activities to which the techniques can be applied. Specifically, the potential influence of AI on those activities was explored. To this end, the SLR was performed with the focus on research studies reporting the implementation of AI techniques in software test automation. Out of 34 primary studies that were included in the final set, 9 distinct software testing activities were identified. These activities had been reportedly improved by applying the AI techniques mostly from the machine learning and computer vision fields. According to the reviewed primary studies, the improvement was achieved in terms of reusability of test cases, manual effort reduction, improved coverage, improved fault and vulnerability detection. Several publicly accessible AI-enhanced tools for software test automation were discovered during the review as well. Their short summary is presented.},
   author = {Anna Trudova and Michal Dolezel and Alena Buchalcevova},
   doi = {10.5220/0009417801810192},
   isbn = {9789897584213},
   booktitle = {ENASE 2020 - Proceedings of the 15th International Conference on Evaluation of Novel Approaches to Software Engineering},
   keywords = {Artificial Intelligence,Literature Study,Software Testing,Test Automation,Test Tools},
   pages = {181-192},
   publisher = {SciTePress},
   title = {Artificial intelligence in software test automation: A systematic literature review},
   year = {2020},
}

@techReport{Garousi2025,
   abstract = {Context: The rise of Artificial Intelligence (AI) in software engineering has led to the development of AI-powered testing tools aimed at supporting human test engineers and improving the efficiency and effectiveness of software testing. However, a systematic evaluation is needed to better understand their capabilities, benefits, and limitations. Objective: This study has two objectives: (1) to conduct a Systematic Tool Review (STR) to identify and categorize AI-powered testing tools based on their AI-driven features; and (2) to perform an empirical study on two representative AI-powered testing tools to assess their effectiveness, efficiency, and limitations in real-world testing scenarios. Method: Our STR identified 56 AI-powered testing tools from industry sources and categorized them based on features such as self-healing tests, visual testing, and AI-powered test generation. Following the STR, two tools-Parasoft Selenic and SmartBear VisualTest-were selected for empirical evaluation using two open-source industrial software systems. Their performance was compared with traditional test automation to evaluate improvements in efficiency and accuracy. Results: The STR provides a comprehensive taxonomy of current AI-powered tools, revealing adoption trends and feature gaps. The empirical study shows that AI-powered testing can enhance test execution speed and reduce test maintenance effort. However, it also highlights critical limitations, including difficulty handling complex UI changes, lack of contextual understanding, and erroneous outputs (due to AI misclassifications). Such errors in test results are especially problematic, as testing itself is meant to identify defects-defects within the testing process undermine its value. Additionally, human oversight remains essential: testers must invest significant effort in reviewing and correcting AI-generated artifacts, which reduces the net efficiency gains promised by AI-powered tools. Conclusion: While AI-powered testing tools hold great promise, further advancements are needed in AI reliability, domain awareness, and reducing the human effort required to oversee AI-generated outputs.},
   author = {Vahid Garousi and Zafar Jafarov and Alper Buğra Keleş and Sevde Değirmenci and Ece Özdemir Testinium AŞ and Ryan Zarringhalami},
   doi = {https://doi.org/10.48550/arXiv.2409.00411},
   keywords = {Software testing,artificial intelligence,empirical study 2,systematic tool review,test automation},
   month = {5},
   title = {AI-powered software testing tools: A systematic review and empirical assessment of their features and limitations},
   url = {https://arxiv.org/abs/2409.00411},
   year = {2025}
}

@inproceedings{Singh2023,
   abstract = {The study investigates the background, advantages, and difficulties of AI-based testing. The use of artificial intelligence (AI) has shown great promise as a means of enhancing software testing procedures. To improve test case generation, bug prediction, and test result analysis, AI-based testing approaches use machine learning, NLP (natural language Processing), GUIs(graphical user interfaces), genetic algorithms, and robotic process automation. We also provide a brief literature review of recent studies in the field, focusing on the various approaches and tools proposed for AI-based software testing. We conclude with a strategy for introducing AI-based testing and a list of possible approaches and resources. Overall, this paper provides a comprehensive survey of AI-based software testing and highlights the potential benefits and challenges of this emerging field.},
   author = {Singh, Akshay and Al-Azzam, Omar},
   doi = {10.5121/csit.2023.132001},
   month = {11},
   pages = {01-12},
   publisher = {Academy and Industry Research Collaboration Center (AIRCC)},
   title = {Artificial Intelligence Applied to Software Testing},
   year = {2023}
}

@article{Li2025,
   abstract = { Testing web forms is an essential activity for ensuring the quality of web applications. It typically involves evaluating the interactions between users and forms. Automated test-case generation remains a challenge for web-form testing: Due to the complex, multi-level structure of web pages, it can be difficult to automatically capture their inherent contextual information for inclusion in the tests. Large Language Models (LLMs) have shown great potential for contextual text generation. This motivated us to explore how they could generate automated tests for web forms, making use of the contextual information within form elements. To the best of our knowledge, no comparative study examining different LLMs has yet been reported for web-form-test generation. To address this gap in the literature, we conducted a comprehensive empirical study investigating the effectiveness of 11 LLMs on 146 web forms from 30 open-source Java web applications. In addition, we propose three HTML-structure-pruning methods to extract key contextual information. The experimental results show that different LLMs can achieve different testing effectiveness, with the GPT-4, GLM-4, and Baichuan2 LLMs generating the best web-form tests. Compared with GPT-4, the other LLMs had difficulty generating appropriate tests for the web forms: Their successfully-submitted rates (SSRs) — the proportions of the LLMs-generated web-form tests that could be successfully inserted into the web forms and submitted — decreased by 9.10% to 74.15%. Our findings also show that, for all LLMs, when the designed prompts include complete and clear contextual information about the web forms, more effective web-form tests were generated. Specifically, when using Parser-Processed HTML for Task Prompt (PH-P), the SSR averaged 70.63%, higher than the 60.21% for Raw HTML for Task Prompt (RH-P) and 50.27% for LLM-Processed HTML for Task Prompt (LH-P). With RH-P, GPT-4's SSR was 98.86%, outperforming models like LLaMa2 (7B) with 34.47% and GLM-4V with 0%. Similarly, with PH-P, GPT-4 reached an SSR of 99.54%, the highest among all models and prompt types. Finally, this paper also highlights strategies for selecting LLMs based on performance metrics, and for optimizing the prompt design to improve the quality of the web-form tests. },
   author = {Tao Li and Chenhui Cui and Rubing Huang and Dave Towey and Lei Ma},
   doi = {10.1145/3735553},
   issn = {1049-331X},
   journal = {ACM Transactions on Software Engineering and Methodology},
   month = {5},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Large Language Models for Automated Web-Form-Test Generation: An Empirical Study},
   url = {https://doi.org/10.1145/3735553},
   year = {2025}
}

@inproceedings{Alshahwan2024,
   abstract = {This paper describes Meta's TestGen-LLM tool, which uses LLMs to automatically improve existing human-written tests. TestGen-LLM verifies that its generated test classes successfully clear a set of filters that assure measurable improvement over the original test suite, thereby eliminating problems due to LLM hallucination. We describe the deployment of TestGen-LLM at Meta test-a-thons for the Instagram and Facebook platforms. In an evaluation on Reels and Stories products for Instagram, 75% of TestGen-LLM's test cases built correctly, 57% passed reliably, and 25% increased coverage. During Meta's Instagram and Facebook test-a-thons, it improved 11.5% of all classes to which it was applied, with 73% of its recommendations being accepted for production deployment by Meta software engineers. We believe this is the first report on industrial scale deployment of LLM-generated code backed by such assurances of code improvement.},
   author = {Nadia Alshahwan and Jubin Chheda and Anastasia Finegenova and Beliz Gokkaya and Mark Harman and Inna Harper and Alexandru Marginean and Shubho Sengupta and Eddy Wang},
   doi = {https://doi.org/10.48550/arXiv.2402.09171},
   isbn = {9798400706585},
   booktitle = {FSE Companion - Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
   keywords = {Automated Test Generation,Genetic Improvement,LLMs,Large Language Models,Unit Testing},
   pages = {185-196},
   publisher = {Association for Computing Machinery, Inc},
   title = {Automated Unit Test Improvement using Large Language Models at Meta},
   url = {https://arxiv.org/abs/2402.09171},
   year = {2024}
}

@article{Schfer2023,
   abstract = {Unit tests play a key role in ensuring the correctness of software. However, manually creating unit tests is a laborious task, motivating the need for automation. Large Language Models (LLMs) have recently been applied to this problem, utilizing additional training or few-shot learning on examples of existing tests. This paper presents a large-scale empirical evaluation on the effectiveness of LLMs for automated unit test generation without additional training or manual effort, providing the LLM with the signature and implementation of the function under test, along with usage examples extracted from documentation. We also attempt to repair failed generated tests by re-prompting the model with the failing test and error message. We implement our approach in TestPilot, a test generation tool for JavaScript that automatically generates unit tests for all API functions in an npm package. We evaluate TestPilot using OpenAI's gpt3.5-turbo LLM on 25 npm packages with a total of 1,684 API functions. The generated tests achieve a median statement coverage of 70.2% and branch coverage of 52.8%, significantly improving on Nessie, a recent feedback-directed JavaScript test generation technique, which achieves only 51.3% statement coverage and 25.6% branch coverage. We also find that 92.8% of TestPilot's generated tests have no more than 50% similarity with existing tests (as measured by normalized edit distance), with none of them being exact copies. Finally, we run TestPilot with two additional LLMs, OpenAI's older code-cushman-002 LLM and the open LLM StarCoder. Overall, we observed similar results with the former (68.2% median statement coverage), and somewhat worse results with the latter (54.0% median statement coverage), suggesting that the effectiveness of the approach is influenced by the size and training set of the LLM, but does not fundamentally depend on the specific model.},
   author = {Max Schäfer and Sarah Nadi and Aryaz Eghbali and Frank Tip},
   doi = {https://doi.org/10.48550/arXiv.2302.06527},
   month = {12},
   title = {An Empirical Evaluation of Using Large Language Models for Automated Unit Test Generation},
   url = {https://arxiv.org/abs/2302.06527},
   year = {2023}
}

@article{Yang2024,
   abstract = {Unit testing is an essential activity in software development for verifying the correctness of software components. However, manually writing unit tests is challenging and time-consuming. The emergence of Large Language Models (LLMs) offers a new direction for automating unit test generation. Existing research primarily focuses on closed-source LLMs (e.g., ChatGPT and CodeX) with fixed prompting strategies, leaving the capabilities of advanced open-source LLMs with various prompting settings unexplored. Particularly, open-source LLMs offer advantages in data privacy protection and have demonstrated superior performance in some tasks. Moreover, effective prompting is crucial for maximizing LLMs' capabilities. In this paper, we conduct the first empirical study to fill this gap, based on 17 Java projects, five widely-used open-source LLMs with different structures and parameter sizes, and comprehensive evaluation metrics. Our findings highlight the significant influence of various prompt factors, show the performance of open-source LLMs compared to the commercial GPT-4 and the traditional Evosuite, and identify limitations in LLM-based unit test generation. We then derive a series of implications from our study to guide future research and practical use of LLM-based unit test generation.},
   author = {Lin Yang and Chen Yang and Shutao Gao and Weijing Wang and Bo Wang and Qihao Zhu and Xiao Chu and Jianyi Zhou and Guangtai Liang and Qianxiang Wang and Junjie Chen},
   doi = {https://doi.org/10.48550/arXiv.2406.18181},
   month = {9},
   title = {An Empirical Study of Unit Test Generation with Large
Language Models.},
   url = {https://arxiv.org/abs/2406.18181},
   year = {2024}
}

@article{RoshniKanth2023,
   author = {Roshni Kanth and R Guru and Madhu B K and Dr.V.S. Akshaya},
   doi = {10.53555/kuey.v30i1.7495},
   issn = {2148-2403},
   journal = {Educational Administration: Theory and Practice},
   month = {1},
   title = {AI Vs. Conventional Testing: A Comprehensive Comparison Of Effectiveness \&Efficiency},
   url = {https://kuey.net/index.php/kuey/article/view/7495},
   year = {2023}
}

@article{Kirinuki2024,
   abstract = {In recent years, large language models (LLMs), such as ChatGPT, have been pivotal in advancing various artificial intelligence applications, including natural language processing and software engineering. A promising yet underexplored area is utilizing LLMs in software testing, particularly in black-box testing. This paper explores the test cases devised by ChatGPT in comparison to those created by human participants. In this study, ChatGPT (GPT-4) and four participants each created black-box test cases for three applications based on specifications written by the authors. The goal was to evaluate the real-world applicability of the proposed test cases, identify potential shortcomings, and comprehend how ChatGPT could enhance human testing strategies. ChatGPT can generate test cases that generally match or slightly surpass those created by human participants in terms of test viewpoint coverage. Additionally, our experiments demonstrated that when ChatGPT cooperates with humans, it can cover considerably more test viewpoints than each can achieve alone, suggesting that collaboration between humans and ChatGPT may be more effective than human pairs working together. Nevertheless, we noticed that the test cases generated by ChatGPT have certain issues that require addressing before use.},
   author = {Hiroyuki Kirinuki and Haruto Tanno},
   doi = {https://doi.org/10.48550/arXiv.2401.13924},
   month = {1},
   title = {ChatGPT and Human Synergy in Black-Box Testing: A Comparative Analysis},
   url = {https://arxiv.org/abs/2401.13924},
   year = {2024}
}

@article{Ricca2025,
   abstract = {Context: Test Automation (TA) techniques are crucial for quality assurance in software engineering but face limitations such as high test suite maintenance costs and the need for extensive programming skills. Artificial Intelligence (AI) offers new opportunities to address these issues through automation and improved practices. Objectives: Given the prevalent usage of AI in industry, sources of truth are held in grey literature as well as the minds of professionals, stakeholders, developers, and end-users. This study surveys grey literature to explore how AI is adopted in TA, focusing on the problems it solves, its solutions, and the available tools. Additionally, the study gathers expert insights to understand AI's current and future role in TA. Methods: We reviewed over 3,600 grey literature sources over five years, including blogs, white papers, and user manuals, and finally filtered 342 documents to develop taxonomies of TA problems and AI solutions. We also cataloged 100 AI-driven TA tools and interviewed five expert software testers to gain insights into AI's current and future role in TA. Results: The study found that manual test code development and maintenance are the main challenges in TA. In contrast, automated test generation and self-healing test scripts are the most common AI solutions. We identified 100 AI-based TA tools, with Applitools, Testim, Functionize, AccelQ, and Mabl being the most adopted in practice. Conclusion: This paper offers a detailed overview of AI's impact on TA through grey literature analysis and expert interviews. It presents new taxonomies of TA problems and AI solutions, provides a catalog of AI-driven tools, and relates solutions to problems and tools to solutions. Interview insights further revealed the state and future potential of AI in TA. Our findings support practitioners in selecting TA tools and guide future research directions.},
   author = {Filippo Ricca and Alessandro Marchetto and Andrea Stocco},
   doi = {https://doi.org/10.48550/arXiv.2408.06224},
   month = {1},
   title = {A Multi-Year Grey Literature Review on AI-assisted Test Automation},
   url = {https://arxiv.org/abs/2408.06224},
   year = {2025}
}

@article{Wang2025,
   abstract = {Unit tests play a vital role in uncovering potential faults in software. While tools like EvoSuite focus on maximizing code coverage, recent advances in large language models (LLMs) have shifted attention toward LLM-based test generation. However, code coverage metrics -- such as line and branch coverage -- remain overly emphasized in reported research, despite being weak indicators of a test suite's fault-detection capability. In contrast, mutation score offers a more reliable and stringent measure, as demonstrated in our findings where some test suites achieve 100% coverage but only 4% mutation score. Although a few studies consider mutation score, the effectiveness of LLMs in killing mutants remains underexplored. In this paper, we propose MUTGEN, a mutation-guided, LLM-based test generation approach that incorporates mutation feedback directly into the prompt. Evaluated on 204 subjects from two benchmarks, MUTGEN significantly outperforms both EvoSuite and vanilla prompt-based strategies in terms of mutation score. Furthermore, MUTGEN introduces an iterative generation mechanism that pushes the limits of LLMs in killing additional mutants. Our study also provide insights into the limitations of LLM-based generation, analyzing the reasons for live and uncovered mutants, and the impact of different mutation operators on generation effectiveness.},
   author = {Guancheng Wang and Qinghua Xu and Lionel C. Briand and Kui Liu},
   doi = {https://doi.org/10.48550/arXiv.2506.02954},
   month = {8},
   title = {Mutation-Guided Unit Test Generation with a Large Language Model},
   url = {https://arxiv.org/abs/2506.02954},
   year = {2025}
}

@article{Junior2023,
   abstract = {This paper presents a detailed case study examining the application of Large Language Models (LLMs) in the construction of test cases within the context of software engineering. LLMs, characterized by their advanced natural language processing capabilities, are increasingly garnering attention as tools to automate and enhance various aspects of the software development life cycle. Leveraging a case study methodology, we systematically explore the integration of LLMs in the test case construction process, aiming to shed light on their practical efficacy, challenges encountered, and implications for software quality assurance. The study encompasses the selection of a representative software application, the formulation of test case construction methodologies employing LLMs, and the subsequent evaluation of outcomes. Through a blend of qualitative and quantitative analyses, this study assesses the impact of LLMs on test case comprehensiveness, accuracy, and efficiency. Additionally, delves into challenges such as model interpretability and adaptation to diverse software contexts. The findings from this case study contributes with nuanced insights into the practical utility of LLMs in the domain of test case construction, elucidating their potential benefits and limitations. By addressing real-world scenarios and complexities, this research aims to inform software practitioners and researchers alike about the tangible implications of incorporating LLMs into the software testing landscape, fostering a more comprehensive understanding of their role in optimizing the software development process.},
   author = {Roberto Francisco de Lima Junior and Luiz Fernando Paes de Barros Presta and Lucca Santos Borborema and Vanderson Nogueira da Silva and Marcio Leal de Melo Dahia and Anderson Carlos Sousa e Santos},
   doi = {https://doi.org/10.48550/arXiv.2312.12598},
   month = {12},
   title = {A Case Study on Test Case Construction with Large Language Models: Unveiling Practical Insights and Challenges},
   url = {https://arxiv.org/abs/2312.12598},
   year = {2023}
}

@article{Rehan2025,
   abstract = {Software testing is critical for ensuring software reliability, with test case generation often being resource-intensive and time-consuming. This study leverages the Llama-2 large language model (LLM) to automate unit test generation for Java focal methods, demonstrating the potential of AI-driven approaches to optimize software testing workflows. Our work leverages focal methods to prioritize critical components of the code to produce more context-sensitive and scalable test cases. The dataset, comprising 25,000 curated records, underwent tokenization and QLoRA quantization to facilitate training. The model was fine-tuned, achieving a training loss of 0.046. These results show the promise of AI-driven test case generation and underscore the feasibility of using fine-tuned LLMs for test case generation, highlighting opportunities for improvement through larger datasets, advanced hyperparameter optimization, and enhanced computational resources. We conducted a human-in-the-loop validation on a subset of unit tests generated by our fined-tuned LLM. This confirms that these tests effectively leverage focal methods, demonstrating the model's capability to generate more contextually accurate unit tests. The work suggests the need to develop novel validation objective metrics specifically tailored for the automation of test cases generated by utilizing large language models. This work establishes a foundation for scalable and efficient software testing solutions driven by artificial intelligence. The data and code are publicly available on GitHub.},
   author = {Shaheer Rehan and Baidaa Al-Bander and Amro Al-Said Ahmad},
   doi = {https://doi.org/10.3390/electronics14071463},
   issn = {20799292},
   issue = {7},
   journal = {Electronics (Switzerland)},
   keywords = {LLM,Llama-2,QLoRA,focal methods,software testing,test case generation,unit testing},
   month = {4},
   publisher = {Multidisciplinary Digital Publishing Institute (MDPI)},
   title = {Harnessing Large Language Models for Automated Software Testing: A Leap Towards Scalable Test Case Generation},
   volume = {14},
   url = {https://www.mdpi.com/2079-9292/14/7/1463},
   year = {2025}
}

@article{Huang2025,
   abstract = {Recently, large language models (LLMs) have shown great promise in automating unit test generation, significantly reducing the manual effort required by developers. To effectively evaluate the capabilities of LLMs in this domain, it is crucial to have a well-designed benchmark that accurately reflects real-world scenarios and mitigates common pitfalls. Existing LLM test generation benchmarks are limited by two critical drawbacks: data contamination and structurally simple function code. As a result, we often cannot rely on the validity of scientific conclusions drawn from empirical studies using these limited benchmarks. The empirical evidence presented may be biased due to contamination and may fail to generalize beyond toy programs due to structural simplicity. To address these problems, we introduce ULT (UnLeakedTestbench), a new benchmark specifically designed for function-level unit test generation from real-world Python functions. ULT is constructed through a multi-stage curation process that ensures high cyclomatic complexity and mitigates test case contamination. With 3,909 carefully selected function-level tasks, ULT provides a more realistic and challenging evaluation of LLMs' test generation capabilities. We also provide PLT (PreLeakedTestbench), a pair benchmark of ULT with leaked tests designed to enable a controlled analysis of memorization versus reasoning in test generation. Our evaluation results demonstrate that ULT is significantly more challenging. For example, test cases generated by LLMs only achieve 41.32\%, 45.10\%, 30.22\%, and 40.21\% for accuracy, statement coverage, branch coverage, and mutation score on average for all LLMs, respectively. These results are substantially lower than the corresponding metrics on TestEval (91.79\%, 92.18\%, 82.04\%, and 49.69\%) and PLT (47.07\%, 55.13\%, 40.07\%, and 50.80\%).},
   author = {Dong Huang and Jie M. Zhang and Mark Harman and Qianru Zhang and Mingzhe Du and See-Kiong Ng},
   doi = {https://doi.org/10.48550/arXiv.2508.00408},
   month = {8},
   title = {Benchmarking LLMs for Unit Test Generation from Real-World Functions},
   url = {https://arxiv.org/abs/2508.00408},
   year = {2025}
}

@article{Bhatia2024,
   abstract = {Generating unit tests is a crucial task in software development, demanding substantial time and effort from programmers. The advent of Large Language Models (LLMs) introduces a novel avenue for unit test script generation. This research aims to experimentally investigate the effectiveness of LLMs, specifically exemplified by ChatGPT, for generating unit test scripts for Python programs, and how the generated test cases compare with those generated by an existing unit test generator (Pynguin). For experiments, we consider three types of code units: 1) Procedural scripts, 2) Function-based modular code, and 3) Class-based code. The generated test cases are evaluated based on criteria such as coverage, correctness, and readability. Our results show that ChatGPT's performance is comparable with Pynguin in terms of coverage, though for some cases its performance is superior to Pynguin. We also find that about a third of assertions generated by ChatGPT for some categories were incorrect. Our results also show that there is minimal overlap in missed statements between ChatGPT and Pynguin, thus, suggesting that a combination of both tools may enhance unit test generation performance. Finally, in our experiments, prompt engineering improved ChatGPT's performance, achieving a much higher coverage.},
   author = {Shreya Bhatia and Tarushi Gandhi and Dhruv Kumar and Pankaj Jalote},
   doi = {https://doi.org/10.48550/arXiv.2312.10622},
   month = {2},
   title = {Unit Test Generation using Generative AI : A Comparative Performance Analysis of Autogeneration Tools},
   url = {https://arxiv.org/abs/2312.10622},
   year = {2024}
}

@article{Gkikopouli2023,
   abstract = {Unit testing plays a crucial role in ensuring the quality and reliability of software systems. However, manual testing can often be a slow and time-consuming process. With current advancements in artificial intelligence (AI), new tools have emerged for automated unit testing to address this issue. But how do these new AI tools compare to conventional automated unit test generation tools? To answer this question, we compared two state-of-the-art conventional unit test tools (EVOSUITE and RANDOOP) with the sole commercially available AI-based unit test tool (DIFFBLUE COVER) for Java. We tested them on 10 sample classes from 3 real-life projects provided by the Defects4J dataset to evaluate their performance regarding code coverage, mutation score, and fault detection. The results showed that EVOSUITE achieved the highest code coverage, averaging 89%, while RANDOOP and DIFFBLUE COVER achieved similar results, averaging 63%. In terms of mutation score, DIFFBLUE COVER had the lowest average score of 40%, while EVOSUITE and RANDOOP scored 67% and 50%, respectively. For fault detection, EVOSUITE and RANDOOP detected a higher number of bugs (7 out of 10 and 5 out of 10, respectively) compared to DIFFBLUE COVER, which found only 4 out of 10. Although the AI-based tool was outperformed in all three criteria, it still shows promise by being able to achieve adequate results, in some cases even surpassing the conventional tools while generating a significantly smaller number of total assertions and more comprehensive tests. Nonetheless, the study acknowledges its limitations in terms of the restricted number of AI-based tools used and the small number of projects utilized from Defects4J.},
   author = {Marios Gkikopouli and Batjigdrel Bataa},
   month = {6},
   pages = {30},
   title = {Empirical Comparison Between Conventional and AI-based Automated Unit Test Generation Tools in Java},
   url = {https://www.diva-portal.org/smash/record.jsf?pid=diva2%3A1764443&dswid=4942},
   year = {2023}
}

@article{Baltes2025,
   abstract = {Large language models (LLMs) are increasingly being integrated into software engineering (SE) research and practice, yet their non-determinism, opaque training data, and evolving architectures complicate the reproduction and replication of empirical studies. We present a community effort to scope this space, introducing a taxonomy of LLM-based study types together with eight guidelines for designing and reporting empirical studies involving LLMs. The guidelines present essential (must) criteria as well as desired (should) criteria and target transparency throughout the research process. Our recommendations, contextualized by our study types, are: (1) to declare LLM usage and role; (2) to report model versions, configurations, and fine-tuning; (3) to document tool architectures; (4) to disclose prompts and interaction logs; (5) to use human validation; (6) to employ an open LLM as a baseline; (7) to use suitable baselines, benchmarks, and metrics; and (8) to openly articulate limitations and mitigations. Our goal is to enable reproducibility and replicability despite LLM-specific barriers to open science. We maintain the study types and guidelines online as a living resource for the community to use and shape (llm-guidelines.org).},
   author = {Sebastian Baltes and Florian Angermeir and Chetan Arora and Marvin Muñoz Barón and Chunyang Chen and Lukas Böhme and Fabio Calefato and Neil Ernst and Davide Falessi and Brian Fitzgerald and Davide Fucci and Marcos Kalinowski and Stefano Lambiase and Daniel Russo and Mircea Lungu and Lutz Prechelt and Paul Ralph and Rijnard van Tonder and Christoph Treude and Stefan Wagner},
   doi = {https://doi.org/10.48550/arXiv.2508.15503},
   month = {9},
   title = {Guidelines for Empirical Studies in Software Engineering involving Large Language Models},
   url = {https://arxiv.org/abs/2508.15503},
   year = {2025}
}

@inproceedings{Trinkenreich2025,
   abstract = {The adoption of Large Language Models (LLMs) is not only transforming software engineering (SE) practice but is also poised to fundamentally disrupt how research is conducted in the field. While perspectives on this transformation range from viewing LLMs as mere productivity tools to considering them revolutionary forces, we argue that the SE research community must proactively engage with and shape the integration of LLMs into research practices, emphasizing human agency in this transformation. As LLMs rapidly become integral to SE research-both as tools that support investigations and as subjects of study-a human-centric perspective is essential. Ensuring human oversight and interpretability is necessary for upholding scientific rigor, fostering ethical responsibility, and driving advancements in the field. Drawing from discussions at the 2nd Copenhagen Symposium on Human-Centered AI in SE, this position paper employs McLuhan's Tetrad of Media Laws to analyze the impact of LLMs on SE research. Through this theoretical lens, we examine how LLMs enhance research capabilities through accelerated ideation and automated processes, make some traditional research practices obsolete, retrieve valuable aspects of historical research approaches, and risk reversal effects when taken to extremes. Our analysis reveals opportunities for innovation and potential pitfalls that require careful consideration. We conclude with a call to action for the SE research community to proactively harness the benefits of LLMs while developing frameworks and guidelines to mitigate their risks, to ensure continued rigor and impact of research in an AI-augmented future.},
   author = {Bianca Trinkenreich and Fabio Calefato and Geir Hanssen and Kelly Blincoe and Marcos Kalinowski and Mauro Pezzè and Paolo Tell and Margaret Anne Storey},
   doi = {10.1145/3696630.3731666},
   isbn = {9798400712760},
   issn = {15397521},
   booktitle = {Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
   keywords = {AI4SE,Generative AI,LLM,McLuhan's Tetrad},
   month = {7},
   pages = {1503-1507},
   publisher = {Association for Computing Machinery},
   title = {Get on the Train or be Left on the Station: Using LLMs for Software Engineering Research},
   url = {https://dl.acm.org/doi/10.1145/3696630.3731666},
   year = {2025}
}

@book{Wohlin2012,
   abstract = {Like other sciences and engineering disciplines, software engineering requires a cycle of model building, experimentation, and learning. Experiments are valuable tools for all software engineers who are involved in evaluating and choosing between different methods, techniques, languages and tools. The purpose of Experimentation in Software Engineering is to introduce students, teachers, researchers, and practitioners to empirical studies in software engineering, using controlled experiments. The introduction to experimentation is provided through a process perspective, and the focus is on the steps that we have to go through to perform an experiment. The book is divided into three parts. The first part provides a background of theories and methods used in experimentation. Part II then devotes one chapter to each of the five experiment steps: scoping, planning, execution, analysis, and result presentation. Part III completes the presentation with two examples. Assignments and statistical material are provided in appendixes. Overall the book provides indispensable information regarding empirical studies in particular for experiments, but also for case studies, systematic literature reviews, and surveys. It is a revision of the authors book, which was published in 2000. In addition, substantial new material, e.g. concerning systematic literature reviews and case study research, is introduced. The book is self-contained and it is suitable as a course book in undergraduate or graduate studies where the need for empirical studies in software engineering is stressed. Exercises and assignments are included to combine the more theoretical material with practical aspects. Researchers will also benefit from the book, learning more about how to conduct empirical studies, and likewise practitioners may use it as a cookbook when evaluating new methods or techniques before implementing them in their organization.},
   author = {Claes Wohlin and Per Runeson and Martin Hst and Magnus C Ohlsson and Bjrn Regnell and Anders Wessln},
   doi = {10.1007/978-3-642-29044-2},
   isbn = {3642290434},
   publisher = {Springer Publishing Company, Incorporated},
   title = {Experimentation in Software Engineering},
   url = {https://dl.acm.org/doi/book/10.5555/2349018},
   year = {2012}
}


@inproceedings{Sjberg2023,
   abstract = {Background: Construct validity concerns the use of indicators to measure a concept that is not directly measurable. Aim: This study intends to identify, categorize, assess and quantify discussions of threats to construct validity in empirical software engineering literature and use the findings to suggest ways to improve the reporting of construct validity issues. Method: We analyzed 83 articles that report human-centric experiments published in five top-tier software engineering journals from 2015 to 2019. The articles' text concerning threats to construct validity was divided into segments (the unit of analysis) based on predefined categories. The segments were then evaluated regarding whether they clearly discussed a threat and a construct. Results: Three-fifths of the segments were associated with topics not related to construct validity. Two-thirds of the articles discussed construct validity without using the definition of construct validity given in the article. The threats were clearly described in more than four-fifths of the segments, but the construct in question was clearly described in only two-thirds of the segments. The construct was unclear when the discussion was not related to construct validity but to other types of validity. Conclusions: The results show potential for improving the understanding of construct validity in software engineering. Recommendations addressing the identified weaknesses are given to improve the awareness and reporting of CV.},
   author = {Dag I.K. Sjøberg and Gunnar R. Bergersen},
   doi = {10.1145/3593434.3593449},
   isbn = {9798400700446},
   booktitle = {ACM International Conference Proceeding Series},
   keywords = {empirical research,measurement,research quality},
   month = {6},
   pages = {205-209},
   publisher = {Association for Computing Machinery},
   title = {Improving the Reporting of Threats to Construct Validity},
   year = {2023}
}

@article{Plaat2025,
   abstract = {Background: There is great interest in agentic LLMs, large language models that act as agents.
Objectives: We review the growing body of work in this area and provide a research agenda.
Methods: Agentic LLMs are LLMs that (1) reason, (2) act, and (3) interact. We organize the literature according to these three categories.
Results: The research in the first category focuses on reasoning, reflection, and retrieval, aiming to improve decision making; the second category focuses on action models, robots, and tools, aiming for agents that act as useful assistants; the third category focuses on multi-agent systems, aiming for collaborative task solving and simulating interaction to study emergent social behavior. We find that works mutually benefit from results in other categories: retrieval enables tool use, reflection improves multi-agent collaboration, and reasoning benefits all categories.
Conclusions: We discuss applications of agentic LLMs and provide an agenda for further research. Important applications are in medical diagnosis, logistics and financial market analysis. Meanwhile, self-reflective agents playing roles and interacting with one another augment the process of scientific research itself. Further, agentic LLMs provide a solution for the problem of LLMs running out of training data: inference-time behavior generates new training states, such that LLMs can keep learning without needing ever larger datasets. We note that there is risk associated with LLM assistants taking action in the real world-safety, liability and security are open problems-while agentic LLMs are also likely to benefit society.},
   author = {Aske Plaat and Max van Duijn and Niki van Stein and Mike Preuss and Peter van der Putten and Kees Joost Batenburg},
   doi = {https://doi.org/10.48550/arXiv.2503.23037},
   month = {11},
   title = {Agentic Large Language Models, a survey},
   url = {https://arxiv.org/abs/2503.23037},
   year = {2025}
}

@article{Liao2025,
   abstract = {Unit testing is essential for software quality assurance, yet writing and maintaining tests remains time-consuming and error-prone. To address this challenge, researchers have proposed various techniques for automating unit test generation, including traditional heuristic-based methods and more recent approaches that leverage large language models (LLMs). However, these existing approaches are inherently path-insensitive because they rely on fixed heuristics or limited contextual information and fail to reason about deep control-flow structures. As a result, they often struggle to achieve adequate coverage, particularly for deep or complex execution paths. In this work, we present a path-sensitive framework, JUnitGenie, to fill this gap by combining code knowledge with the semantic capabilities of LLMs in guiding context-aware unit test generation. After extracting code knowledge from Java projects, JUnitGenie distills this knowledge into structured prompts to guide the generation of high-coverage unit tests. We evaluate JUnitGenie on 2,258 complex focal methods from ten real-world Java projects. The results show that JUnitGenie generates valid tests and improves branch and line coverage by 29.60% and 31.00% on average over both heuristic and LLM-based baselines. We further demonstrate that the generated test cases can uncover real-world bugs, which were later confirmed and fixed by developers.},
   author = {Dianshu Liao and Xin Yin and Shidong Pan and Chao Ni and Zhenchang Xing and Xiaoyu Sun},
   doi = {https://doi.org/10.48550/arXiv.2509.23812},
   month = {10},
   title = {Navigating the Labyrinth: Path-Sensitive Unit Test Generation with Large Language Models},
   url = {http://arxiv.org/abs/2509.23812},
   year = {2025}
}
